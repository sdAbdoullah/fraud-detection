# ğŸ—ï¸ Architecture - DÃ©tection Proactive de Fraude Bancaire

**Documentation technique complÃ¨te du systÃ¨me**

---

## ğŸ“‹ Table des matiÃ¨res

1. [Vue d'ensemble](#vue-densemble)
2. [Architecture systÃ¨me](#architecture-systÃ¨me)
3. [Pipeline ML](#pipeline-ml)
4. [Application Streamlit](#application-streamlit)
5. [IntÃ©gration IA GÃ©nÃ©rative](#intÃ©gration-ia-gÃ©nÃ©rative)
6. [Stack technologique](#stack-technologique)
7. [Flux de donnÃ©es](#flux-de-donnÃ©es)
8. [Performance & ScalabilitÃ©](#performance--scalabilitÃ©)
9. [SÃ©curitÃ©](#sÃ©curitÃ©)

---

## Vue d'ensemble

### Objectif architectural
CrÃ©er une **solution modulaire et dÃ©ployable** combinant :
- ğŸ¤– Pipeline ML robuste (entraÃ®nement + prÃ©diction)
- ğŸ§  IA gÃ©nÃ©rative pour explainability (Gemini API)
- ğŸ“Š Application web interactive (Streamlit)
- ğŸ” Gestion sÃ©curisÃ©e des donnÃ©es et API keys

### Principes de design
- âœ… **SÃ©paration des prÃ©occupations** - ML, UI, API distinctes
- âœ… **ModularitÃ©** - Composants indÃ©pendants et rÃ©utilisables
- âœ… **ScalabilitÃ©** - Architecture prÃªte pour volume production
- âœ… **Robustesse** - Gestion d'erreurs, fallbacks, logging
- âœ… **MaintenabilitÃ©** - Code documentÃ©, tests, CI/CD ready

---

## Architecture systÃ¨me

### Diagram macro

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SYSTÃˆME COMPLET                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    LAYER 1: DATA & TRAINING
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Jupyter Notebook (fraud_detection_final.ipynb)            â”‚
    â”‚  â€¢ Load data (creditcard.csv)                              â”‚
    â”‚  â€¢ EDA & Feature Analysis                                  â”‚
    â”‚  â€¢ Preprocessing (StandardScaler)                          â”‚
    â”‚  â€¢ SMOTE (rÃ©Ã©quilibre)                                     â”‚
    â”‚  â€¢ Train 3 models (RF, XGB, LGBM)                         â”‚
    â”‚  â€¢ Evaluation & Selection                                  â”‚
    â”‚  â€¢ Save artifacts (model.pkl, scaler.pkl)                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
    LAYER 2: MODELS & INFERENCE
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  XGBoost Model (production-ready)                           â”‚
    â”‚  â€¢ Saved as: models/xgboost_model.pkl                     â”‚
    â”‚  â€¢ Input: 30 features (V1-V28, Time, Amount)             â”‚
    â”‚  â€¢ Output: Fraud probability [0-1]                        â”‚
    â”‚  â€¢ ROC-AUC: 0.9725                                        â”‚
    â”‚  â€¢ Rappel: 84.46%                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
    LAYER 3: APPLICATION & UI
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Streamlit App (app.py)                                    â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”â”‚
    â”‚  â”‚Dashboardâ”‚ Alertes â”‚Analyse  â”‚SynthÃ¨se â”‚ScÃ©nariosâ”‚Exportsâ”‚â”‚
    â”‚  â”‚Onglet 1 â”‚Onglet 2 â”‚Onglet 3 â”‚Onglet 4 â”‚Onglet 5â”‚Onglet6â”‚â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜â”‚
    â”‚  â€¢ Load model + scaler                                     â”‚
    â”‚  â€¢ Display real-time metrics                               â”‚
    â”‚  â€¢ Filter & analyze transactions                           â”‚
    â”‚  â€¢ Call Gemini API for explanations                       â”‚
    â”‚  â€¢ Export to Excel                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
    LAYER 4: AI EXPLANATION
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Google Gemini 2.5 Flash API                              â”‚
    â”‚  â€¢ Receive transaction data + ML score                     â”‚
    â”‚  â€¢ Generate textual explanations                           â”‚
    â”‚  â€¢ Produce recommendations (BLOCK/VERIFY/MONITOR)         â”‚
    â”‚  â€¢ Create synthetic fraud scenarios                        â”‚
    â”‚  â€¢ Generate daily risk summaries                           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
    OUTPUT: ANALYST DECISION
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â€¢ Dashboard view with alerts
    â€¢ Gemini explanations for each transaction
    â€¢ Excel exports for reporting
    â€¢ Recommendations for actions
```

---

## Pipeline ML

### Phase 1: Data Loading & EDA

```python
# Input: creditcard.csv (284,807 rows Ã— 31 columns)
#
# Variables:
# â”œâ”€ V1-V28: PCA components (anonymized)
# â”œâ”€ Time: Seconds since first transaction
# â”œâ”€ Amount: Transaction amount (USD)
# â””â”€ Class: Target (0=legitimate, 1=fraud)

# Output: Understanding of data distribution
# â”œâ”€ Class balance: 99.83% vs 0.17%
# â”œâ”€ Amount statistics: mean 88.35, median varies
# â”œâ”€ Correlations: V11, V4, V2 â†’ strongest with fraud
# â””â”€ Missing values: None (perfect data)
```

### Phase 2: Preprocessing

```python
# StandardScaler normalization
# â”œâ”€ Input: Amount [0, 25691.16], Time [0, 172800]
# â””â”€ Output: Amount, Time ~ N(0, 1)
#
# Train/Test split (stratified)
# â”œâ”€ Train: 199,364 (70%) - maintain 0.17% fraud ratio
# â””â”€ Test: 85,443 (30%) - maintain 0.17% fraud ratio
#
# SMOTE (on training set only)
# â”œâ”€ Before: 199,020 legitimate vs 344 fraud
# â”œâ”€ After: 199,020 legitimate vs 199,020 synthetic fraud
# â””â”€ k_neighbors=5, sampling_strategy=1.0
```

### Phase 3: Model Training

```python
# Random Forest
# â”œâ”€ n_estimators=100
# â”œâ”€ max_depth=15
# â”œâ”€ ROC-AUC: 0.9690
# â””â”€ Rappel: 78.38%

# XGBoost â­ (SELECTED)
# â”œâ”€ n_estimators=200
# â”œâ”€ max_depth=5
# â”œâ”€ learning_rate=0.01
# â”œâ”€ ROC-AUC: 0.9725 (BEST)
# â””â”€ Rappel: 84.46% (BEST)

# LightGBM
# â”œâ”€ n_estimators=150
# â”œâ”€ max_depth=7
# â”œâ”€ ROC-AUC: 0.9636
# â””â”€ Rappel: 83.11%
```

### Phase 4: Evaluation

```python
# Metrics computed on test set (85,443 transactions):
#
# Confusion Matrix
# â”œâ”€ TP: 125 (fraudes correctement dÃ©tectÃ©es)
# â”œâ”€ FP: 28,000 (fausses alertes)
# â”œâ”€ FN: 23 (fraudes manquÃ©es)
# â””â”€ TN: 57,295 (lÃ©gitimes correctement classÃ©es)
#
# Derived metrics
# â”œâ”€ Accuracy: 99.63% (global correctness)
# â”œâ”€ Precision: 29.98% (TP / (TP+FP))
# â”œâ”€ Rappel: 84.46% (TP / (TP+FN)) â† KEY METRIC
# â”œâ”€ F1-score: 0.4467
# â””â”€ ROC-AUC: 0.9725 (area under ROC curve)
#
# Feature Importance
# â”œâ”€ V11: 25%
# â”œâ”€ V4: 18%
# â”œâ”€ V2: 15%
# â”œâ”€ V14: 12%
# â””â”€ V12: 10%
```

### Phase 5: Model Serialization

```python
# Save to disk for production
pickle.dump(xgb_model, open('models/xgboost_model.pkl', 'wb'))
pickle.dump(scaler, open('models/standard_scaler.pkl', 'wb'))
json.dump(model_metadata, open('models/model_info.json', 'w'))
```

---

## Application Streamlit

### Architecture modulaire

```
app/
â”œâ”€â”€ app.py                    # Main entry point
â”œâ”€â”€ config.py                 # Configuration centralized
â”œâ”€â”€ utils.py                  # Utility functions
â””â”€â”€ components/
    â”œâ”€â”€ dashboard.py          # Tab 1: KPIs & overview
    â”œâ”€â”€ alerts.py             # Tab 2: Real-time alerts
    â”œâ”€â”€ analysis.py           # Tab 3: Detailed analysis
    â”œâ”€â”€ gemini_integration.py # Tab 4: AI summaries
    â”œâ”€â”€ scenarios.py          # Tab 5: Synthetic scenarios
    â””â”€â”€ exports.py            # Tab 6: Excel exports
```

### Tab Architecture

```
TAB 1: DASHBOARD
â”œâ”€ Metrics
â”‚  â”œâ”€ Total transactions
â”‚  â”œâ”€ Number of alerts
â”‚  â”œâ”€ Total amount
â”‚  â””â”€ At-risk amount
â””â”€ Visualizations
   â”œâ”€ Amount distribution (histogram)
   â”œâ”€ Fraud probability distribution
   â”œâ”€ Alerts by city
   â””â”€ Activity by hour

TAB 2: REAL-TIME ALERTS
â”œâ”€ Filters
â”‚  â”œâ”€ Risk level (CRITICAL/HIGH/MEDIUM)
â”‚  â”œâ”€ City
â”‚  â””â”€ Merchant type
â””â”€ Results table
   â”œâ”€ Transaction ID
   â”œâ”€ Amount
   â”œâ”€ Time
   â”œâ”€ Fraud probability
   â”œâ”€ [Gemini Analysis] button
   â””â”€ [Details] button

TAB 3: DETAILED ANALYSIS
â”œâ”€ Amount boxplots (all vs alerts)
â”œâ”€ Amount by merchant type
â””â”€ City Ã— Risk heatmap

TAB 4: AI SYNTHESIS
â”œâ”€ [Generate Global Summary] button
â””â”€ Gemini response
   â”œâ”€ Risk overview
   â”œâ”€ Key patterns
   â”œâ”€ Operational recommendations
   â””â”€ Strategic insights

TAB 5: SYNTHETIC SCENARIOS
â”œâ”€ Slider: number of scenarios (1-10)
â”œâ”€ [Generate] button
â””â”€ Results
   â”œâ”€ Scenario 1 (high amount)
   â”œâ”€ Scenario 2 (sequence)
   â””â”€ [Download CSV] button

TAB 6: EXPORTS
â”œâ”€ Alert table
â””â”€ [Export to Excel] button
```

### State Management

```python
# Session state for interactivity
st.session_state.model          # Cached XGBoost model
st.session_state.scaler         # Cached StandardScaler
st.session_state.transactions   # Loaded transactions
st.session_state.predictions    # ML predictions
st.session_state.alerts         # Filtered alerts
st.session_state.gemini_cache   # Cached Gemini responses
```

---

## IntÃ©gration IA GÃ©nÃ©rative

### Pipeline Gemini

```
Transaction Data
    â†“
Prompt Engineering
    â”œâ”€ System message: "You are a fraud detection expert"
    â”œâ”€ User message: "Analyze this transaction..."
    â”œâ”€ Constraints: 2-3 sentences analysis
    â”œâ”€ Format: Recommendation + Signals
    â””â”€ Temperature: 0.7 (balanced creativity)
    â†“
API Call
    â””â”€ google.generativeai.GenerativeModel('gemini-2.5-flash')
    â†“
Response Processing
    â”œâ”€ Extract recommendation (BLOCK/VERIFY/MONITOR)
    â”œâ”€ Extract signals (key indicators)
    â”œâ”€ Format for display
    â””â”€ Cache for identical inputs
    â†“
Display to Analyst
    â””â”€ Rich formatted text with recommendations
```

### Example Prompt

```python
system_prompt = """You are an expert in bank fraud detection. 
Your role is to provide clear, actionable explanations for flagged transactions.
Analyze the data and provide:
1. Brief analysis (2-3 sentences)
2. Recommendation (BLOCK/VERIFY/MONITOR)
3. Key signals that triggered the alert
Keep explanations concise and operationally focused."""

user_prompt = f"""Analyze this transaction:
- ID: {tx_id}
- Amount: ${amount}
- Hour: {hour}
- City: {city}
- Merchant: {merchant_type}
- ML Score: {fraud_prob:.1%}

Provide analysis in French."""
```

### Error Handling

```python
try:
    response = gemini_model.generate_content(prompt)
    return response.text
except APIError as e:
    logger.error(f"Gemini API error: {e}")
    return fallback_explanation(transaction_data)
except RateLimitError as e:
    logger.warning(f"Rate limit: {e}")
    return cached_response if available else fallback
```

---

## Stack technologique

### Backend

| Component | Version | Purpose |
|-----------|---------|---------|
| **Python** | 3.8+ | Language |
| **Pandas** | 1.5+ | Data manipulation |
| **Scikit-learn** | 1.0+ | Preprocessing (StandardScaler, SMOTE) |
| **XGBoost** | 1.5+ | ML model |
| **Imbalanced-learn** | 0.9+ | SMOTE implementation |
| **Joblib** | 1.0+ | Model serialization |

### Frontend

| Component | Version | Purpose |
|-----------|---------|---------|
| **Streamlit** | 1.20+ | Web application |
| **Plotly** | 5.0+ | Interactive visualizations |
| **Pandas** | 1.5+ | Data display |
| **Openpyxl** | 3.7+ | Excel export |

### AI & APIs

| Component | Version | Purpose |
|-----------|---------|---------|
| **Google Generative AI** | Latest | Gemini API client |
| **Python-dotenv** | 0.20+ | Environment variables |

### DevOps

| Component | Purpose |
|-----------|---------|
| **Git** | Version control |
| **GitHub Actions** | CI/CD pipeline |
| **Docker** | Containerization (optional) |
| **pytest** | Unit testing |

### Complete Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          TECHNOLOGY STACK                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                    â”‚
â”‚  Frontend       Data         ML         AI         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚  Streamlit      Pandas       XGBoost    Gemini    â”‚
â”‚  Plotly         NumPy        SMOTE      API       â”‚
â”‚  Openpyxl       Scikit-learn  RF        LLM       â”‚
â”‚                 Joblib       LGBM                 â”‚
â”‚                                                    â”‚
â”‚  Infrastructure:                                  â”‚
â”‚  â€¢ Python 3.8+                                    â”‚
â”‚  â€¢ pip/conda                                      â”‚
â”‚  â€¢ Git                                            â”‚
â”‚  â€¢ GitHub Actions (CI/CD)                         â”‚
â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Flux de donnÃ©es

### 1. Training Flow

```
Raw Data (CSV)
    â†“ [Load]
Pandas DataFrame (284,807 rows)
    â†“ [Explore]
Statistical Analysis
    â†“ [Clean]
Processed Data
    â†“ [Normalize]
StandardScaler applied
    â†“ [Split]
Train (70%) | Test (30%)
    â†“ [SMOTE on Train]
Balanced Train Set
    â†“ [Train 3 Models]
Random Forest | XGBoost | LightGBM
    â†“ [Evaluate on Test]
Metrics: ROC-AUC, Recall, Precision, F1
    â†“ [Select Best]
XGBoost (ROC-AUC 0.9725)
    â†“ [Serialize]
model.pkl | scaler.pkl | metadata.json
```

### 2. Prediction Flow

```
New Transaction
    â†“ [Load Model + Scaler]
XGBoost Model (from pickle)
    â†“ [Normalize Features]
StandardScaler.transform()
    â†“ [Predict]
probability = model.predict_proba()
    â†“ [Threshold Check]
if proba > 0.50 â†’ Alert
    â†“ [Call Gemini]
generate_content(transaction + score)
    â†“ [Format Response]
explanation + recommendation
    â†“ [Display in Streamlit]
Analyst sees alert with explanation
```

### 3. Application Flow

```
User opens Streamlit app
    â†“
Load cached model & scaler
    â†“
Load transaction data
    â†“
Generate predictions for all transactions
    â†“
Filter alerts (proba > 0.50)
    â†“
Display dashboard with metrics
    â†“
User selects tab
    â”œâ”€ Dashboard: Show KPIs
    â”œâ”€ Alerts: Show filtered alerts
    â”œâ”€ Analysis: Show detailed plots
    â”œâ”€ Synthesis: Call Gemini for summary
    â”œâ”€ Scenarios: Generate synthetic cases
    â””â”€ Exports: Download Excel
```

---

## Performance & ScalabilitÃ©

### Benchmarks (current version)

| Operation | Time | Resources |
|-----------|------|-----------|
| Load model + scaler | ~100ms | 50MB RAM |
| Predict 5,000 transactions | ~200ms | 100MB RAM |
| Generate Gemini explanation | 1-2s | API call |
| Render dashboard | ~500ms | CPU + RAM |
| Export 1,000 rows to Excel | ~1s | Disk I/O |

### Scalability considerations

**Current bottleneck**: Gemini API latency (1-2s per call)

**Solutions for scaling**:
1. **Batch processing**: Queue Gemini requests, process asynchronously
2. **Caching**: Store Gemini responses for identical transactions
3. **Load balancing**: Distribute API calls across multiple API keys
4. **Background jobs**: Use Celery for async Gemini calls

**Future improvements**:
```python
# V2 Architecture with async processing
from celery import Celery

@app.task
def generate_gemini_explanation_async(transaction_id, data):
    # Call Gemini in background
    explanation = gemini_model.generate_content(prompt)
    # Store in database
    db.save(transaction_id, explanation)
    # Notify UI
    return explanation
```

---

## SÃ©curitÃ©

### API Key Management

```python
# âœ… CORRECT: Use environment variables
from dotenv import load_dotenv
load_dotenv()
api_key = os.getenv('GEMINI_API_KEY')

# âŒ INCORRECT: Hardcode keys
api_key = "sk_xxxxxxxxxxxxx"  # NEVER!
```

### Secrets in .gitignore

```
.env                    # Environment variables
.env.local              # Local overrides
secrets.json            # Secret configs
credentials.json        # API credentials
*.pem                   # Private keys
```

### Data Privacy

```python
# Before storing/displaying transactions:
def anonymize_transaction(tx):
    tx['city'] = hashlib.md5(tx['city'].encode()).hexdigest()[:8]
    # Don't store raw transaction details
    return tx
```

### Rate Limiting for Gemini

```python
from functools import lru_cache
import time

@lru_cache(maxsize=1000)
def get_gemini_explanation(tx_hash):
    # Cache identical transactions
    # Avoid duplicate API calls
    return gemini_response

# Implement rate limiting
max_requests_per_minute = 60
```

---

## Deployment Architecture

### Local Development

```
Machine
â”œâ”€ Python venv
â”œâ”€ Models (pickle files)
â”œâ”€ Streamlit app
â””â”€ Gemini API (cloud)
```

### Production (Cloud - Optional)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        PRODUCTION DEPLOYMENT         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                      â”‚
â”‚  Load Balancer (CloudFlare)         â”‚
â”‚         â†“                            â”‚
â”‚  Web Server (Streamlit Cloud)       â”‚
â”‚  â”œâ”€ app.py                          â”‚
â”‚  â”œâ”€ models/ (S3 storage)            â”‚
â”‚  â””â”€ requirements.txt                â”‚
â”‚         â†“                            â”‚
â”‚  External APIs                      â”‚
â”‚  â”œâ”€ Google Gemini API               â”‚
â”‚  â”œâ”€ Database (PostgreSQL)           â”‚
â”‚  â””â”€ Storage (S3/GCS)                â”‚
â”‚                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Monitoring & Logging

### Key Metrics to Monitor

```python
import logging

logger = logging.getLogger(__name__)

# Model performance
logger.info(f"Model accuracy: {accuracy:.4f}")
logger.info(f"Model ROC-AUC: {roc_auc:.4f}")

# API calls
logger.info(f"Gemini API calls: {gemini_calls}")
logger.warning(f"API errors: {api_errors}")

# Application performance
logger.info(f"Prediction time: {pred_time}ms")
logger.info(f"Memory usage: {memory}MB")
```

---

## Conclusion

Cette architecture est **modulaire, scalable et production-ready**. Les trois composants principaux (ML, App, AI) sont indÃ©pendants mais intÃ©grÃ©s de faÃ§on cohÃ©rente.

**Prochaines Ã©tapes** :
1. ImplÃ©menter async Gemini calls
2. Ajouter base de donnÃ©es pour logging
3. DÃ©ployer sur Streamlit Cloud
4. Configurer CI/CD avec GitHub Actions
